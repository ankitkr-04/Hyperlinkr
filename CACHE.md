# üöÄ Cache Architecture Deep Dive

## Multi-Layer Cache Implementation

Hyperlinkr implements a sophisticated **4-layer caching system** designed for maximum performance and fault tolerance:

### üèóÔ∏è Cache Layers

```rust
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Cache Request Flow                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. L1 Cache (Moka)    ‚îÇ In-memory, NUMA-aware, TinyLFU     ‚îÇ
‚îÇ 2. Bloom Filter       ‚îÇ Probabilistic existence check       ‚îÇ
‚îÇ 3. L2 Cache (Moka)    ‚îÇ Larger in-memory cache              ‚îÇ
‚îÇ 4. DragonflyDB        ‚îÇ Redis-compatible persistence        ‚îÇ
‚îÇ 5. Sled (Optional)    ‚îÇ Embedded disk storage               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üß† L1 Cache (Primary)
- **Technology**: Moka with TinyLFU eviction policy
- **NUMA Optimization**: Auto-detects NUMA topology for optimal memory allocation
- **Performance**: Sub-microsecond access times
- **Capacity**: Configurable (default: 10K entries)

```rust
// L1 Cache with NUMA awareness
#[cfg(feature = "libnuma")]
unsafe {
    if lib_numa::numa_available() >= 0 {
        let _ = libnuma_sys::numa_preferred();
    }
}
```

### üå∏ Bloom Filter (Existence Check)
- **Purpose**: Prevents unnecessary L2/DB lookups for non-existent keys
- **Implementation**: Sharded atomic Bloom filter (16 shards)
- **Hash Function**: DefaultHasher for fast key distribution
- **False Positive Rate**: Tunable based on expected load

```rust
// Sharded Bloom filter for better concurrency
pub struct CacheBloom {
    shards: Vec<Arc<AtomicBloomShard>>,
    shard_count: usize, // 16 shards for CPU core optimization
}
```

### üíæ L2 Cache (Secondary)
- **Technology**: Moka with larger capacity
- **Purpose**: Overflow from L1, reduces DB pressure
- **Capacity**: Configurable (default: 100K entries)
- **TTL**: Configurable expiration times

### üóÑÔ∏è DragonflyDB (Persistence)
- **Type**: Redis-compatible in-memory database
- **Circuit Breaker**: Fault tolerance with automatic failover
- **Connection Pooling**: Optimized connection management
- **Performance**: Handles 50K+ RPS sustained load

### üíø Sled Storage (Optional Cold Storage)
- **Purpose**: Persistent disk storage for rarely accessed data
- **Flush Strategy**: Periodic background flushing (configurable interval)
- **Recovery**: Automatic cache warming from disk on restart

## üîÑ Cache Operations

### GET Operation Flow
```rust
async fn get(key: &str) -> Result<String, AppError> {
    // 1. Check L1 Cache (fastest)
    if let Some(val) = l1.get(key).await {
        return Ok(val); // ~100ns access time
    }
    
    // 2. Bloom filter check (prevents unnecessary lookups)
    if !bloom.contains(key.as_bytes()) {
        return Err(NotFound); // ~10ns check
    }
    
    // 3. Check L2 Cache
    if let Some(val) = l2.get(key).await {
        l1.insert(key, val.clone()).await; // Promote to L1
        return Ok(val);
    }
    
    // 4. Check DragonflyDB
    if let Ok(val) = dragonfly.get(key).await {
        // Parallel cache population
        future::try_join_all([
            l1.insert(key, val.clone()),
            l2.insert(key, val.clone()),
        ]).await?;
        return Ok(val);
    }
    
    // 5. Check Sled (optional cold storage)
    if use_sled {
        if let Ok(val) = sled.get(key).await {
            // Warm all cache layers
            future::try_join_all([
                dragonfly.set_ex(key, val, ttl),
                l1.insert(key, val.clone()),
                l2.insert(key, val.clone()),
                bloom.insert(key.as_bytes()),
            ]).await?;
            return Ok(val);
        }
    }
    
    Err(NotFound)
}
```

### INSERT Operation Flow
```rust
async fn insert(key: String, value: String) -> Result<(), AppError> {
    // 1. Write to DragonflyDB first (durability)
    dragonfly.set_ex(&key, &value, ttl).await?;
    
    // 2. Parallel cache population
    future::try_join_all([
        l1.insert(key.clone(), value.clone()),
        l2.insert(key.clone(), value.clone()),
        bloom.insert(key.as_bytes()),
        sled.set_ex(key, value, ttl), // Optional
    ]).await?;
    
    Ok(())
}
```

## üìä Performance Characteristics

### Cache Hit Rates (Typical)
- **L1 Cache**: 85-90% hit rate
- **L2 Cache**: 8-12% hit rate  
- **DragonflyDB**: 2-5% hit rate
- **Combined**: >95% total hit rate

### Latency Profile
```
Operation           | Latency    | Throughput
--------------------|------------|-------------
L1 Cache Hit        | ~100ns     | 10M+ ops/sec
L2 Cache Hit        | ~500ns     | 2M+ ops/sec
Bloom Filter Check  | ~10ns      | 100M+ ops/sec
DragonflyDB Hit     | ~100Œºs     | 50K+ ops/sec
Sled Read           | ~1ms       | 1K+ ops/sec
```

## üõ†Ô∏è Configuration

### Cache Tuning Parameters
```toml
[cache]
# L1 Cache (fastest, smallest)
l1_capacity = 10000
ttl_seconds = 3600

# L2 Cache (larger, still fast)
l2_capacity = 100000

# Bloom Filter (probability tuning)
bloom_bits = 2097152        # 2MB bit array
bloom_expected = 100000     # Expected unique keys
bloom_shards = 16           # CPU core optimization

# DragonflyDB
redis_pool_size = 16        # Connection pool
redis_command_timeout_secs = 5

# Sled (optional)
use_sled = true
sled_flush_ms = 1000        # Background flush interval
sled_cache_bytes = 134217728 # 128MB
```

## üö¶ Circuit Breaker Pattern

The cache implements a circuit breaker for DragonflyDB connections:

```rust
pub struct CircuitBreaker {
    state: AtomicU8,           // Open/Closed/HalfOpen
    failure_count: AtomicU32,   // Current failures
    max_failures: u32,          // Trip threshold
    retry_interval: Duration,   // Cooldown period
}
```

### States:
- **Closed**: Normal operation, requests pass through
- **Open**: Failures exceeded threshold, requests fail fast
- **Half-Open**: Testing if service recovered

## üî¨ Metrics & Monitoring

Cache performance is tracked via Prometheus metrics:

```rust
// Metrics tracked
- cache_hits_total{layer="l1|l2|dragonfly|sled"}
- cache_latency_seconds{operation="get|insert|delete"}
- bloom_filter_checks_total
- circuit_breaker_state{state="open|closed|half_open"}
- sled_flush_count_total
```

## üéØ Design Decisions

### Why Multi-Layer?
1. **Performance**: L1 provides ultra-fast access for hot data
2. **Capacity**: L2 extends cache size without L1 overhead
3. **Efficiency**: Bloom filter prevents expensive misses
4. **Durability**: DragonflyDB ensures data persistence
5. **Recovery**: Sled provides cold storage and crash recovery

### Why Moka over alternatives?
- **TinyLFU**: Superior hit rate vs LRU/LFU
- **Async-first**: Native async/await support
- **NUMA-aware**: Optimized for multi-core systems
- **Low overhead**: Minimal memory and CPU impact

### Why Sharded Bloom Filter?
- **Concurrency**: Reduces lock contention
- **Scalability**: Better performance on multi-core systems
- **Cache-friendly**: Each shard fits in CPU cache

This architecture delivers the **50K RPS** and **416K codegen ops/sec** performance demonstrated in benchmarks while maintaining data consistency and fault tolerance.
